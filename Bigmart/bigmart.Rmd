---
  output: rmarkdown::github_document
    
---
  
  <!-- README.md is generated from README.Rmd. Please edit that file -->
  
```{r, echo = FALSE}
knitr::opts_chunk$set(collapse=TRUE, comment="##", fig.retina=2, fig.path = "figures/README-")
```

#Supervised Learning Project: Iris Classification
The data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store have been defined. The aim is to build a predictive model and find out the sales of each product at a particular store.

Using this model, BigMart will try to understand the properties of products and stores which play a key role in increasing sales.

Please note that some of the data may have missing values as some stores might not report all the data due to technical glitches. Hence, this will be treated accordingly to ensure the best outcome possible. Here are the variables in the data set:

* Item_Identifier: Unique product ID
* Item_Weight: Weight of product
* Item_Fat_Content: Whether the product is low fat or not
* Item_Visibility: The % of total display area of all products in a store allocated to the particular product
* Item_Type: The category to which the product belongs
* Item_MRP: Maximum Retail Price (list price) of the product
* Outlet_Identifier: Unique store ID
* Outlet_Establishment_Year: The year in which store was established
* Outlet_Size: The size of the store in terms of ground area covered
* Outlet_Location_Type: The type of city in which the store is located
* Outlet_Type: Whether the outlet is just a grocery store or some sort of supermarket
* Item_Outlet_Sales: Sales of the product in the particulat store. This is the outcome variable to be predicted.

The variables are mainly from 2 category, the item itself or the features of the outlet. From these variables, we are able to come up with hypothesis to understand relationship between the input variables and target variable. 

1. Food type of item would have higher sales.
2. Low fat has better sales because customers prefer low fat than regular.
3. Items which are more visible has higher sales.
4. The bigger the size of the outlet would have higher sales. 
5. Outlets in areas with higher income would have higher sales.

##Load Packages and Data
In this section, we will load all the necessary packages for this project.

```{r,echo=TRUE,warning=FALSE,message=FALSE}  
#Import Packages
library(tidyverse)
library(caret)
library(party)
library(randomForest)
library(VIM)
library(mice)
library(psych)
library(MASS)
library(Metrics)
library(dplyr)
library(rpart)

#load Data
df <- read.csv("~/Programming/R/3 - Projects/Project 13 - Bigmart/Data/Train_UWu5bXk.txt")

#First six rows on the data set
head(df)

```

##Data Exploration
During this stage, we will investigate the data set even futher to have a better understanding of the data. 

```{r,echo=TRUE}
#To see the shape of the data
glimpse(df)
```

There is a total of 8,523 observations and 12 variables in the data set. There are 6 categorical variables, 4 number variables and 1 date vairable. 

```{r,echo=TRUE}
#Have a look at the summary statistics of input variables
summary(df)
```
From the summary we can see that some of the variables require transformation such as Item_Fat_Content because it has the same meaning between different category. Item_Type has 16 different category and requires to be narrowed down to be able to use for the modelling phase later. We can see some missing data and also a category missing in Outlet_Size. We can also see the summary statistics for the target variable. 

##Data Visualization
Data Visualization helps us see how the target variable react with the input variables

###Outlet Sales
```{r,echo=TRUE}
#Histogram
ggplot(df,aes(x=Item_Outlet_Sales))+
  geom_histogram(fill='purple',bins = 30)
```

In the histogram we can see that most of the sales is on the left side of the histogram. This is expected because histogram involving spending does usually has this type of histogram. 

###Item Type
Since there 16 types, we are going to transform into 3 main values which are Food, Drinks and Non-Consume. 
```{r,echo=TRUE}
df$New_Type <- ifelse(df$Item_Type %in% c('Soft Drinks','Hard Drinks'),'Drinks',ifelse(df$Item_Type %in% c('Health and Hygiene','Household','Others'),'Non-Consume','Food'))

df$New_Type <- as.factor(df$New_Type)

ggplot(df,aes(x=New_Type))+
  geom_bar()
```

After the transformation, we have created a bar plot and it is obvious that Food has the most count. 

```{r,echo=TRUE}
ggplot(df,aes(x=New_Type,y=Item_Outlet_Sales))+
  geom_boxplot()
```

From the boxplot, we can see that the Drinks' median is lower than both Food and Non-Consume. 

###Fat Content
Previously, we have seen that the values in this column are the same and requires transformation.

```{r,echo=TRUE}
df$Item_Fat_Content <-ifelse(df$Item_Fat_Content %in% c('reg','Regular'),'Regular','Low Fat')
df$Item_Fat_Content <-ifelse(df$New_Type%in%'Non-Consume','Non-Consume',df$Item_Fat_Content)
df$Item_Fat_Content <- as.factor(df$Item_Fat_Content)

ggplot(df,aes(x=Item_Fat_Content))+
  geom_bar()
```

From the bar chart, we can see that Low Fat has the most counts compared to the other two. 

```{r,echo=TRUE}
a = ggplot(df,aes(x=Item_Fat_Content,y=Item_Outlet_Sales))+
  geom_boxplot()
```

From the boxplot,we can see that the median for all three values are quite similar which means that this variable doesn't have a huge impact on the sales of the outlet.

###Item Visibility
Some of values in this variable is 0 but that does not make any sense therefore we are going to consider this as missing value and impute it with the mean. 
```{r,echo=TRUE}
df$Item_Visibility[df$Item_Visibility==0]<-NA
df$Item_Visibility[is.na(df$Item_Visibility)] = mean(df$Item_Visibility, na.rm=TRUE)
```

From the bar chart, we can see that Low Fat has the most counts compared to the other two. 

```{r,echo=TRUE}
ggplot(df,aes(x=Item_Visibility,y=Item_Outlet_Sales))+
  geom_point()
```

From the scatterplot, we can see that there is no relationship between the two variables.

###Outlet Size
Outlet size seems to be missing a value and this would be a problem if we do not change it now. 
```{r,echo=TRUE}
table(df$Outlet_Type,df$Outlet_Size)
```
Looking at the table, is it quite obvious that the missing value is in the small due to its similarity of outlet type breakdown. 

```{r,echo=TRUE}
df$Outlet_Size[df$Outlet_Size %in%  ''] <- 'Small'
df$Outlet_Size <- as.factor(df$Outlet_Size)
ggplot(df,aes(x=Outlet_Size,y=Item_Outlet_Sales))+
  geom_boxplot()
```

From the boxplot,it is clear smaller outlet has lower sales but medium outlets have a higher median than the rest.

###Outlet Location
The outlets have 3 main location type with all different income groups.
```{r,echo=TRUE}
barchart(df$Outlet_Location_Type)
```
We can see that Bigmart has the most outlets in tier 3 locations followed by tier 2 and 1. 

```{r,echo=TRUE}
ggplot(df,aes(x=Outlet_Location_Type,y=Item_Outlet_Sales))+
  geom_boxplot()
```

From the boxplot,we can see that tier 2 locations have the highest sales followed by tier 3 then tier 1. This indicates that Bigmart customers are mainly from tier 2 locations. 

##Data splitting
Before training a machine learning model, it is crucial to split the data set into two groups so that we are able to test out model. The data is split into training and testing data. The training data consist of 70% of the data set. 

```{r,echo=TRUE}
#data split
new_data = df%>% dplyr::select(-Item_Identifier,-Outlet_Identifier)
trainIndex = createDataPartition(new_data$Item_Outlet_Sales,p=0.7,times=1,list=F)
training = new_data[trainIndex,]
testing = new_data[-trainIndex,]
```

##Data Modelling
For data modelling, we will take the training data and use logistic regression and decision tree to create the model. 

```{r,echo=TRUE,results='hide'}
model_lm1 <-lm(Item_Outlet_Sales ~+Item_Visibility + Item_MRP + Outlet_Establishment_Year + Outlet_Size + Outlet_Location_Type,data=training)
summary(model_lm1)

tree_1 <- rpart(Item_Outlet_Sales~.,data=training)
summary(tree_1)

```

##Model Evaluation 
Once we have done the modelling, we will use mean squared error(MSE) to evaluate both models. Model with the lowest MSE is the best performing model. 

```{r,echo=TRUE}
predict_1 <- predict(model_lm1,testing)
mse(predict_1,testing$Item_Outlet_Sales)
plot(predict_1,testing$Item_Outlet_Sales,xlab="Predict",ylab='Actual')
abline(a=0,b=1,col='red')
```

Logistic regression has an MSE of 1622357 and from the plot we can see how the model performs. 

```{r,echo=TRUE}
tree_22 <- predict(tree_1,testing)
mse(tree_22,testing$Item_Outlet_Sales)
```

Decision tree has an MSE of `r mse(tree_22,testing$Item_Outlet_Sales)`. 

##Summary
From the model, we can conclude that some variables are more important than others in predicting the sales of each outlet. This is crucial for Bigmart to understand as the model might change if they decide to operate different. 